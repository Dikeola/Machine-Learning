Implementation with Hyperopt

Step 1: Define the Search Space

python
from hyperopt import hp

space = {
    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1)),
    'max_depth': hp.quniform('max_depth', 3, 15, 1),
    'n_estimators': hp.quniform('n_estimators', 50, 500, 50),
    'min_child_weight': hp.uniform('min_child_weight', 1, 10),
    'subsample': hp.uniform('subsample', 0.5, 1)
}
Step 2: Create Objective Function

python
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier

def objective(params):
    # Convert parameters to proper types
    params = {
        'learning_rate': float(params['learning_rate']),
        'max_depth': int(params['max_depth']),
        'n_estimators': int(params['n_estimators']),
        'min_child_weight': params['min_child_weight'],
        'subsample': params['subsample']
    }
    
    model = XGBClassifier(**params, random_state=42)
    score = cross_val_score(model, X_train, y_train, 
                          scoring='accuracy', cv=5).mean()
    
    # Hyperopt minimizes, so return negative accuracy
    return {'loss': -score, 'status': STATUS_OK}
Step 3: Run Optimization

python
from hyperopt import fmin, tpe, Trials

trials = Trials()  # Stores optimization history

best = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,  # Tree-structured Parzen Estimator
    max_evals=100,
    trials=trials,
    rstate=np.random.RandomState(42)
Analyzing Results

1. View Best Parameters

python
print("Best parameters found:")
print(best)
2. Visualize Optimization Progress

python
import matplotlib.pyplot as plt

losses = [trial['result']['loss'] for trial in trials.trials]
plt.plot(losses)
plt.title('Convergence Over Time')
plt.xlabel('Iteration')
plt.ylabel('Negative Accuracy')
plt.show()
3. Parameter Importance Analysis

python
from hyperopt.plotting import main_plot_history, main_plot_vars

main_plot_history(trials)
main_plot_vars(trials)
Advanced Techniques

1. Early Stopping

python
from hyperopt import early_stop

best = fmin(
    ...,
    early_stop_fn=early_stop.no_progress_loss(100)  # Stop if no improvement for 100 iters
)
2. Conditional Parameters

python
space = hp.choice('classifier_type', [
    {
        'type': 'xgboost',
        'learning_rate': hp.loguniform('xgb_lr', np.log(0.001), np.log(0.1)),
        'max_depth': hp.quniform('xgb_depth', 3, 15, 1)
    },
    {
        'type': 'random_forest',
        'n_estimators': hp.quniform('rf_n', 50, 500, 50),
        'max_features': hp.uniform('rf_max_feat', 0.1, 1)
    }
])
3. Parallel Execution

python
from hyperopt import SparkTrials

# Run on Spark cluster
spark_trials = SparkTrials(parallelism=4)
best = fmin(..., trials=spark_trials)
Practical Considerations

Parameter Space Design:
Use log scales for parameters like learning rates (hp.loguniform)
Quantize integer parameters (hp.quniform)
Consider dependencies between parameters
Evaluation Budget:
Start with 50-100 evaluations for initial exploration
Increase to 200-500 for fine-tuning
Surrogate Model Choice:
TPE (default) works well for most cases
Consider Gaussian Processes for small parameter spaces
Random Forest surrogates for categorical parameters
Result Analysis:
Track optimization history
Visualize parameter distributions of best trials
Check for convergence
Comparison to Other Methods

Method	Pros	Cons
Grid Search	Exhaustive, simple	Computationally expensive
Random Search	Parallelizable, better scaling	No learning from trials
Bayesian Opt	Sample-efficient, informed	Complex setup, sequential nature
Evolutionary	Good for mixed parameter types	Requires large population sizes
Bayesian optimization typically requires 10-100x fewer evaluations than random search to find comparable or better solutions.
